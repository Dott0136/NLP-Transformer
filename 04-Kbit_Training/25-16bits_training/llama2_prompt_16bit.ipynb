{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# prompt 实战——————已GPTQ量化"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GPTQConfig"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ds = Dataset.load_from_disk(\"../data/alpaca_data_zh/\")\n",
    "ds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ds[:3]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# print(len(\"以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"D:\\pycharm\\modelscope\\Pretrained_models\\modelscope\\Llama-2-7b-ms\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/modelscope/Pretrained_models/modelscope/Llama-2-7b-ms\")\n",
    "tokenizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tokenizer.padding_side = \"right\"  # 一定要设置padding_side为right，否则batch大于1时可能不收敛"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tokenizer.pad_token_id = 2\n",
    "'''\n",
    "# LLaMA2 的特殊标记 ID\n",
    "SPECIAL_TOKEN_IDS = {\n",
    "    0: \"<unk>\",    # 未知标记\n",
    "    1: \"<s>\",      # 开始标记（bos）\n",
    "    2: \"</s>\",     # 结束标记（eos）\n",
    "    32000: \"<pad>\" # 填充标记（但需要手动添加）\n",
    "}\n",
    "# 当设置 pad_token_id = 2 时：\n",
    "# 实际上是将 </s>（结束标记）用作填充标记\n",
    "# 这是合理的，因为 </s> 通常出现在序列末尾\n",
    "\n",
    "所以也可以写成：tokenizer.pad_token = tokenizer.eos_token\n",
    "'''"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 1024    # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join([\"Human: \" + example[\"instruction\"], example[\"input\"]]).strip() + \"\\n\\nAssistant: \", add_special_tokens=False)\n",
    "    response = tokenizer(example[\"output\"], add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(tokenized_ds[\"input_ids\"][0])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# tokenizer(\"abc \" + tokenizer.eos_token)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "tokenizer.decode(tokenized_ds[\"input_ids\"][0])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# tokenizer(\"呀\", add_special_tokens=False) # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_ds[1][\"labels\"])))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "calibration_data = [example[\"instruction\"] + example[\"input\"] + example[\"output\"] for example in ds.select(range(128))]\n",
    "# 使用Transformers的GPTQConfig\n",
    "GPTQ_config = GPTQConfig(\n",
    "    bits=4,  # 4位量化\n",
    "    dataset=calibration_data,  # 校准数据集\n",
    "    tokenizer=tokenizer,\n",
    "    group_size=128,\n",
    "    desc_act=False,\n",
    "    sym=True,\n",
    "    true_sequential=True,\n",
    "    #以下为GPTQ必写字段，否则卡死\n",
    "    loss_type='causal_lm',\n",
    "    disable_exllama=True,\n",
    "    inject_fused_attention=False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "# 多卡情况，可以去掉device_map=\"auto\"，否则会将模型拆开\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/root/autodl-tmp/modelscope/Pretrained_models/modelscope/Llama-2-7b-ms\", low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=\"auto\",quantization_config=GPTQConfig)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.dtype"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prompt"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Step1 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from peft import PromptTuningConfig, get_peft_model, TaskType, PromptTuningInit\n",
    "config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM,   #告诉模型这是生成式任务（对话、续写等）\n",
    "                            prompt_tuning_init=PromptTuningInit.TEXT,   #指定提示词初始化方式\n",
    "                            prompt_tuning_init_text=\"下面是一段人与机器人的对话。\",   #初始化的具体文本内容,这就是一个 Hard Prompt\n",
    "                            num_virtual_tokens=len(tokenizer(\"下面是一段人与机器人的对话。\")[\"input_ids\"]),   #自动计算提示词的token数量\n",
    "                            tokenizer_name_or_path=\"/root/autodl-tmp/modelscope/Pretrained_models/modelscope/Llama-2-7b-ms\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Step2 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = get_peft_model(model, config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "#config",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.enable_input_require_grads() # 开启梯度检查点时，要执行该方法\n",
    "'''\n",
    "gradient_checkpointng\n",
    "梯度检查点是一种用时间换空间的优化技术，用于解决深度学习训练中的显存瓶颈问题核心思想,使训练多花30%的时间，但是显存需求更低2/3\n",
    "• 在标准训练中，前向传播需要保存所有中间激活值用于反向传播\n",
    "• 这些激活值占用大量显存，尤其是大型模型和长序列\n",
    "梯度检查点只保存部分激活值，在反向传播时重新计算丢失的激活值\n",
    "\n",
    "float32：32位浮点数，占用4字节\n",
    "float16：16位浮点数，占用2字节\n",
    "'''"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# model = model.half()  # 当整个模型都是半精度时，需要将adam_epsilon调大\n",
    "# torch.tensor(1e-8).half() "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.print_trainable_parameters()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dl = DataLoader(tokenized_ds, batch_size=2, collate_fn=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ipt = next(enumerate(dl))[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ipt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model(**ipt.to(\"cuda\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./chatbot\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_ds.select(range(6000)),\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "ipt = tokenizer(\"Human: {}\\n{}\".format(\"你好\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors=\"pt\").to(model.device)\n",
    "tokenizer.decode(model.generate(**ipt, max_length=512, do_sample=True, eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

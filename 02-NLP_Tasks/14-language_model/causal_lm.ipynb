{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因果语言模型训练实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T09:51:30.438357Z",
     "start_time": "2025-11-27T09:51:13.129776Z"
    }
   },
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, BloomForCausalLM"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T09:51:44.412378Z",
     "start_time": "2025-11-27T09:51:44.371607Z"
    }
   },
   "source": [
    "ds = Dataset.load_from_disk(\"./wiki_cn_filtered/\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T09:51:45.893763Z",
     "start_time": "2025-11-27T09:51:45.877551Z"
    }
   },
   "source": [
    "ds"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'completion'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T09:51:49.215490Z",
     "start_time": "2025-11-27T09:51:49.209941Z"
    }
   },
   "source": [
    "ds[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'wikipedia.zh2307',\n",
       " 'completion': \"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安交通大学的博物馆，馆长是锺明善。\\n历史\\n2004年9月20日开始筹建，2013年4月8日正式建成开馆，位于西安交通大学兴庆校区陕西省西安市咸宁西路28号。建筑面积6,800平米，展厅面积4,500平米，馆藏文物4,900余件。包括历代艺术文物馆、碑石书法馆、西部农民画馆、邢良坤陶瓷艺术馆、陕西秦腔博物馆和书画展厅共五馆一厅。\\n营业时间\\n* 周一至周六：上午九点至十二点，下午一点至五点\\n* 周日闭馆\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T09:52:30.452590Z",
     "start_time": "2025-11-27T09:52:29.300667Z"
    }
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/bloom-389m-zh\")\n",
    "\n",
    "def process_func(examples):\n",
    "    contents = [e + tokenizer.eos_token for e in examples[\"completion\"]]        #在每个文本样本的末尾添加结束标记<eos>\n",
    "    return tokenizer(contents, max_length=384, truncation=True)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T09:52:34.883195Z",
     "start_time": "2025-11-27T09:52:32.741385Z"
    }
   },
   "source": [
    "tokenized_ds = ds.map(process_func, batched=True, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22326e4f519a484db2838955f5fc94d8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T12:19:04.893730Z",
     "start_time": "2025-11-27T12:19:04.877761Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "'''\n",
    "掩码语言模型\n",
    "# mlm=True: 双向注意力\n",
    "# 可以同时看到左右上下文\n",
    "注意力: [今天, [MASK], 很] → 预测\"天气\"\n",
    "\n",
    "因果语言模型\n",
    "# mlm=False: 单向注意力（因果注意力）\n",
    "# 只能看到左边的词\n",
    "注意力: [今天] → 预测\"天气\"\n",
    "注意力: [今天, 天气] → 预测\"很\"\n",
    "注意力: [今天, 天气, 很] → 预测\"好\"\n",
    "'''\n",
    "dl = DataLoader(tokenized_ds, batch_size=2, collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T12:19:09.228863Z",
     "start_time": "2025-11-27T12:19:09.200963Z"
    }
   },
   "source": [
    "next(enumerate(dl))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " {'input_ids': tensor([[    3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3, 13110, 34800,\n",
       "          13535,   916, 33156,    10,   256,   576,   387,   479,   681,  5453,\n",
       "          10955,   915, 24124,  5317, 13110,  6573, 20757, 13535,   355,  5358,\n",
       "           1490,   583, 28056,  1407,  3855,   671,  6113,   189,  6732,  4302,\n",
       "           9488,  3434,  6900,  1322,   355, 37336,  9825,  4608, 13461,  1359,\n",
       "           5358,   355,  5317, 13110, 34800,  4433,  7189, 25722, 29747, 13110,\n",
       "           1498, 12047,  6347, 23563,  2139,  2066,   420, 29288,    25,    15,\n",
       "           7635, 39288,   355,  1484,  5835,  6272,    23,    15,  4180, 39288,\n",
       "            355,  5358,  4516, 11621,    23,    15, 10641,  4887,  1712,   420,\n",
       "           2450, 31163,  8085, 11621,  5358,   553,  9888,  2731, 21335,  5358,\n",
       "            553,  9876, 14011,  4434,  5358,   553, 21484,  4514, 17170, 25871,\n",
       "           8085,  5358,   553, 17489,  6945, 11097, 13535,   641, 33623,  1484,\n",
       "           5835,  1689,  2063,  5358,   569,  5835,   671, 16225,  3422,   189,\n",
       "             13, 23158, 27894, 33227,  1022, 11396,  3347,  1813,  1504,  6566,\n",
       "           1813,   355,  9155,  8633,  1504,  2063,  1813,   189,    13, 23158,\n",
       "            813,  7817,  5358,     2],\n",
       "         [  586, 11835,   739,   355,  8417,  2300,   916, 16892,  5077,   580,\n",
       "          15549,  1434,   996,   307,   387,   355,  1997,  8236,   775,  8417,\n",
       "           2152,  6496, 12176,  3728,  7692,  1503,  1528,  1014, 17887, 18001,\n",
       "           3250,   355,  6250,  3896,  2670, 20882, 16080, 14005,  3993,  1503,\n",
       "          12295,  8930,  3250,   420,  4208,  4326,  5367,  8417,  2152,  4632,\n",
       "          37591,   355,  1379,  8417,  2300, 32824,  3250,  9015, 18945, 16714,\n",
       "           5908, 13551, 30330, 23756,  2152, 15976,   657,  5923,  8417,   586,\n",
       "          16080,  1528,  8941,  5917, 14035,  5895, 14092,  4353, 29445,   355,\n",
       "           8790, 21595,  1450, 15911, 31116, 10345, 29940, 10874,  1125,  4829,\n",
       "          16080,  7093, 22939,   737,   262,   387,    72,   272,   831,   455,\n",
       "             72,   915,  8860, 20725,  1934,  1084,  5478,   420,  4415,  8417,\n",
       "          26263, 12726,   553, 10875, 34820,   355,  1266,  5498,   586, 14907,\n",
       "          32795, 11835,   904, 19934,  1528, 19531, 20517,  1349, 19472, 28879,\n",
       "            671,  8417, 26263, 17020,  5963, 22388, 11900, 12669, 13240,  1042,\n",
       "           9783,   355,  7242,   714,  1806,   775,  6500,   355, 11526, 10185,\n",
       "           1293,  1665, 15984,  7092,  1617,  8417,  2300,   420, 19972, 25622,\n",
       "          10875, 17500, 26523,  2391,  8417,  2300,   355,  6751,  2836, 13539,\n",
       "           8247,   373, 30201,  5498,   420,  8417,  2300,   586,  8523, 19358,\n",
       "           1298, 12176, 30939, 10739,   964,  4318, 10875,   420, 11900, 16080,\n",
       "            904,  9783,   355, 22464,  9658,   355,  8417,  2300, 13561,  2054,\n",
       "           4983,  4829, 30800,  7262,   420, 11394,  8417, 35143, 11937, 15682,\n",
       "           8417,  2300,  7283, 10875,   355,  1016,  4179,  5039, 14027, 26215,\n",
       "          26835,   671, 15095,   189,  1165, 15095, 11900,  6184,  1125,  3244,\n",
       "           3687,   622,  8785,  1121,   891, 13765,   671, 10199,   189,    13,\n",
       "            210,  6940,  3728,  9552,  6082,  8417,  2300,   916,  4375,   714,\n",
       "           3679,  1806, 10567,   915,   189,    13,   210, 16131, 11835,  8417,\n",
       "           2300,   189,    13, 24075,  3728,  8417,  2300,   916,  4829,  3687,\n",
       "           9000, 27689,  8417,  2300,  1300, 11243,  2062, 28431, 27689, 11835,\n",
       "           8417,  2300, 13224,  4829,  3687, 15964,   915,   189,    13, 27340,\n",
       "          11835,  8417,  2300,   189,    13, 27340,  3982,  3728,  8417,  2300,\n",
       "            916,  4375,  2450,  3272,  1234, 19083,   553,  3512,  3121,  1728,\n",
       "            641,  3092,  2113,  7843,   915,   189,    13,   210, 15402,  8417,\n",
       "           2300,   189,    13, 10057,   108, 12693, 14624, 29379,  4719,  6533,\n",
       "            739,   916, 16148, 10981, 21350,  9067,  1203,  8931,  1258, 11835,\n",
       "           4719,  6533,   739, 20393,   189,    13,   210, 19546,  1517, 11835,\n",
       "           8417,  2300,   189,    13,   210, 23928,   168,   117,   245,  6279,\n",
       "            114,   240,   170,   100,   124,   168,   117,   228,  6279,   100,\n",
       "            124,   168,   117,   228,   171,   238,   224, 41356,   236, 24175,\n",
       "          11082, 10981, 21350,  9067]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 13110, 34800,\n",
       "          13535,   916, 33156,    10,   256,   576,   387,   479,   681,  5453,\n",
       "          10955,   915, 24124,  5317, 13110,  6573, 20757, 13535,   355,  5358,\n",
       "           1490,   583, 28056,  1407,  3855,   671,  6113,   189,  6732,  4302,\n",
       "           9488,  3434,  6900,  1322,   355, 37336,  9825,  4608, 13461,  1359,\n",
       "           5358,   355,  5317, 13110, 34800,  4433,  7189, 25722, 29747, 13110,\n",
       "           1498, 12047,  6347, 23563,  2139,  2066,   420, 29288,    25,    15,\n",
       "           7635, 39288,   355,  1484,  5835,  6272,    23,    15,  4180, 39288,\n",
       "            355,  5358,  4516, 11621,    23,    15, 10641,  4887,  1712,   420,\n",
       "           2450, 31163,  8085, 11621,  5358,   553,  9888,  2731, 21335,  5358,\n",
       "            553,  9876, 14011,  4434,  5358,   553, 21484,  4514, 17170, 25871,\n",
       "           8085,  5358,   553, 17489,  6945, 11097, 13535,   641, 33623,  1484,\n",
       "           5835,  1689,  2063,  5358,   569,  5835,   671, 16225,  3422,   189,\n",
       "             13, 23158, 27894, 33227,  1022, 11396,  3347,  1813,  1504,  6566,\n",
       "           1813,   355,  9155,  8633,  1504,  2063,  1813,   189,    13, 23158,\n",
       "            813,  7817,  5358,     2],\n",
       "         [  586, 11835,   739,   355,  8417,  2300,   916, 16892,  5077,   580,\n",
       "          15549,  1434,   996,   307,   387,   355,  1997,  8236,   775,  8417,\n",
       "           2152,  6496, 12176,  3728,  7692,  1503,  1528,  1014, 17887, 18001,\n",
       "           3250,   355,  6250,  3896,  2670, 20882, 16080, 14005,  3993,  1503,\n",
       "          12295,  8930,  3250,   420,  4208,  4326,  5367,  8417,  2152,  4632,\n",
       "          37591,   355,  1379,  8417,  2300, 32824,  3250,  9015, 18945, 16714,\n",
       "           5908, 13551, 30330, 23756,  2152, 15976,   657,  5923,  8417,   586,\n",
       "          16080,  1528,  8941,  5917, 14035,  5895, 14092,  4353, 29445,   355,\n",
       "           8790, 21595,  1450, 15911, 31116, 10345, 29940, 10874,  1125,  4829,\n",
       "          16080,  7093, 22939,   737,   262,   387,    72,   272,   831,   455,\n",
       "             72,   915,  8860, 20725,  1934,  1084,  5478,   420,  4415,  8417,\n",
       "          26263, 12726,   553, 10875, 34820,   355,  1266,  5498,   586, 14907,\n",
       "          32795, 11835,   904, 19934,  1528, 19531, 20517,  1349, 19472, 28879,\n",
       "            671,  8417, 26263, 17020,  5963, 22388, 11900, 12669, 13240,  1042,\n",
       "           9783,   355,  7242,   714,  1806,   775,  6500,   355, 11526, 10185,\n",
       "           1293,  1665, 15984,  7092,  1617,  8417,  2300,   420, 19972, 25622,\n",
       "          10875, 17500, 26523,  2391,  8417,  2300,   355,  6751,  2836, 13539,\n",
       "           8247,   373, 30201,  5498,   420,  8417,  2300,   586,  8523, 19358,\n",
       "           1298, 12176, 30939, 10739,   964,  4318, 10875,   420, 11900, 16080,\n",
       "            904,  9783,   355, 22464,  9658,   355,  8417,  2300, 13561,  2054,\n",
       "           4983,  4829, 30800,  7262,   420, 11394,  8417, 35143, 11937, 15682,\n",
       "           8417,  2300,  7283, 10875,   355,  1016,  4179,  5039, 14027, 26215,\n",
       "          26835,   671, 15095,   189,  1165, 15095, 11900,  6184,  1125,  3244,\n",
       "           3687,   622,  8785,  1121,   891, 13765,   671, 10199,   189,    13,\n",
       "            210,  6940,  3728,  9552,  6082,  8417,  2300,   916,  4375,   714,\n",
       "           3679,  1806, 10567,   915,   189,    13,   210, 16131, 11835,  8417,\n",
       "           2300,   189,    13, 24075,  3728,  8417,  2300,   916,  4829,  3687,\n",
       "           9000, 27689,  8417,  2300,  1300, 11243,  2062, 28431, 27689, 11835,\n",
       "           8417,  2300, 13224,  4829,  3687, 15964,   915,   189,    13, 27340,\n",
       "          11835,  8417,  2300,   189,    13, 27340,  3982,  3728,  8417,  2300,\n",
       "            916,  4375,  2450,  3272,  1234, 19083,   553,  3512,  3121,  1728,\n",
       "            641,  3092,  2113,  7843,   915,   189,    13,   210, 15402,  8417,\n",
       "           2300,   189,    13, 10057,   108, 12693, 14624, 29379,  4719,  6533,\n",
       "            739,   916, 16148, 10981, 21350,  9067,  1203,  8931,  1258, 11835,\n",
       "           4719,  6533,   739, 20393,   189,    13,   210, 19546,  1517, 11835,\n",
       "           8417,  2300,   189,    13,   210, 23928,   168,   117,   245,  6279,\n",
       "            114,   240,   170,   100,   124,   168,   117,   228,  6279,   100,\n",
       "            124,   168,   117,   228,   171,   238,   224, 41356,   236, 24175,\n",
       "          11082, 10981, 21350,  9067]])})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T12:22:36.108035Z",
     "start_time": "2025-11-27T12:22:36.095728Z"
    }
   },
   "source": "tokenizer.pad_token, tokenizer.pad_token_id     #对填充token和结尾标token进行token化，并且从tokenizer这个典型的分词器词汇表里面查到pad和eos这两个token的id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:29:06.607151Z",
     "start_time": "2025-11-27T14:29:06.592881Z"
    }
   },
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:35:48.732209Z",
     "start_time": "2025-11-27T14:33:25.211717Z"
    }
   },
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Langboat/bloom-389m-zh\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   5%|5         | 83.9M/1.56G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98fb1a486997453997b9d2f2f21b3172"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(4475246 bytes read, 1468636631 more expected)', IncompleteRead(4475246 bytes read, 1468636631 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIncompleteRead\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\urllib3\\response.py:779\u001B[0m, in \u001B[0;36mHTTPResponse._error_catcher\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 779\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m    781\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m SocketTimeout \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001B[39;00m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;66;03m# there is yet no clean way to get at it from this context.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\urllib3\\response.py:925\u001B[0m, in \u001B[0;36mHTTPResponse._raw_read\u001B[1;34m(self, amt, read1)\u001B[0m\n\u001B[0;32m    915\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    916\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menforce_content_length\n\u001B[0;32m    917\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength_remaining \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    923\u001B[0m         \u001B[38;5;66;03m# raised during streaming, so all calls with incorrect\u001B[39;00m\n\u001B[0;32m    924\u001B[0m         \u001B[38;5;66;03m# Content-Length are caught.\u001B[39;00m\n\u001B[1;32m--> 925\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m IncompleteRead(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp_bytes_read, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength_remaining)\n\u001B[0;32m    926\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m read1 \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[0;32m    927\u001B[0m     (amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength_remaining \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(data)\n\u001B[0;32m    928\u001B[0m ):\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    931\u001B[0m     \u001B[38;5;66;03m# `http.client.HTTPResponse`, so we close it here.\u001B[39;00m\n\u001B[0;32m    932\u001B[0m     \u001B[38;5;66;03m# See https://github.com/python/cpython/issues/113199\u001B[39;00m\n",
      "\u001B[1;31mIncompleteRead\u001B[0m: IncompleteRead(4475246 bytes read, 1468636631 more expected)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mProtocolError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\requests\\models.py:820\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[1;34m()\u001B[0m\n\u001B[0;32m    819\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 820\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    821\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\urllib3\\response.py:1091\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[1;34m(self, amt, decode_content)\u001B[0m\n\u001B[0;32m   1090\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 1091\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1093\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data:\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\urllib3\\response.py:1008\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[1;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[0;32m   1004\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m<\u001B[39m amt \u001B[38;5;129;01mand\u001B[39;00m data:\n\u001B[0;32m   1005\u001B[0m     \u001B[38;5;66;03m# TODO make sure to initially read enough data to get past the headers\u001B[39;00m\n\u001B[0;32m   1006\u001B[0m     \u001B[38;5;66;03m# For example, the GZ file header takes 10 bytes, we don't want to read\u001B[39;00m\n\u001B[0;32m   1007\u001B[0m     \u001B[38;5;66;03m# it one byte at a time\u001B[39;00m\n\u001B[1;32m-> 1008\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raw_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1009\u001B[0m     decoded_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decode(data, decode_content, flush_decoder)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\urllib3\\response.py:903\u001B[0m, in \u001B[0;36mHTTPResponse._raw_read\u001B[1;34m(self, amt, read1)\u001B[0m\n\u001B[0;32m    901\u001B[0m fp_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m--> 903\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_catcher():\n\u001B[0;32m    904\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp_read(amt, read1\u001B[38;5;241m=\u001B[39mread1) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fp_closed \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\contextlib.py:153\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[1;34m(self, typ, value, traceback)\u001B[0m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 153\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraceback\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\urllib3\\response.py:803\u001B[0m, in \u001B[0;36mHTTPResponse._error_catcher\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    802\u001B[0m         arg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection broken: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 803\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ProtocolError(arg, e) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    805\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (HTTPException, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mProtocolError\u001B[0m: ('Connection broken: IncompleteRead(4475246 bytes read, 1468636631 more expected)', IncompleteRead(4475246 bytes read, 1468636631 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mChunkedEncodingError\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLangboat/bloom-389m-zh\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[1;32m--> 564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    565\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    566\u001B[0m     )\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    570\u001B[0m )\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\transformers\\modeling_utils.py:3457\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   3454\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3455\u001B[0m         \u001B[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001B[39;00m\n\u001B[0;32m   3456\u001B[0m         filename \u001B[38;5;241m=\u001B[39m _add_variant(WEIGHTS_NAME, variant)\n\u001B[1;32m-> 3457\u001B[0m         resolved_archive_file \u001B[38;5;241m=\u001B[39m cached_file(\n\u001B[0;32m   3458\u001B[0m             pretrained_model_name_or_path, filename, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcached_file_kwargs\n\u001B[0;32m   3459\u001B[0m         )\n\u001B[0;32m   3460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resolved_archive_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m filename \u001B[38;5;241m==\u001B[39m _add_variant(WEIGHTS_NAME, variant):\n\u001B[0;32m   3461\u001B[0m     \u001B[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001B[39;00m\n\u001B[0;32m   3462\u001B[0m     resolved_archive_file \u001B[38;5;241m=\u001B[39m cached_file(\n\u001B[0;32m   3463\u001B[0m         pretrained_model_name_or_path,\n\u001B[0;32m   3464\u001B[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001B[0;32m   3465\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcached_file_kwargs,\n\u001B[0;32m   3466\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\transformers\\utils\\hub.py:402\u001B[0m, in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    399\u001B[0m user_agent \u001B[38;5;241m=\u001B[39m http_user_agent(user_agent)\n\u001B[0;32m    400\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    401\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[1;32m--> 402\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    403\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    404\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    405\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    406\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    407\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    408\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    409\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    410\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    415\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    416\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    417\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[0;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\huggingface_hub\\file_download.py:1007\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001B[0m\n\u001B[0;32m    987\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _hf_hub_download_to_local_dir(\n\u001B[0;32m    988\u001B[0m         \u001B[38;5;66;03m# Destination\u001B[39;00m\n\u001B[0;32m    989\u001B[0m         local_dir\u001B[38;5;241m=\u001B[39mlocal_dir,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1004\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[0;32m   1005\u001B[0m     )\n\u001B[0;32m   1006\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1007\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_hf_hub_download_to_cache_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1008\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Destination\u001B[39;49;00m\n\u001B[0;32m   1009\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1010\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# File info\u001B[39;49;00m\n\u001B[0;32m   1011\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1012\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1013\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1014\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1015\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# HTTP info\u001B[39;49;00m\n\u001B[0;32m   1016\u001B[0m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1017\u001B[0m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1018\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1019\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1020\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1021\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Additional options\u001B[39;49;00m\n\u001B[0;32m   1022\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1023\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1024\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\huggingface_hub\\file_download.py:1168\u001B[0m, in \u001B[0;36m_hf_hub_download_to_cache_dir\u001B[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[0m\n\u001B[0;32m   1165\u001B[0m \u001B[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001B[39;00m\n\u001B[0;32m   1167\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m WeakFileLock(lock_path):\n\u001B[1;32m-> 1168\u001B[0m     \u001B[43m_download_to_tmp_and_move\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1169\u001B[0m \u001B[43m        \u001B[49m\u001B[43mincomplete_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblob_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.incomplete\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1170\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdestination_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblob_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1171\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl_to_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl_to_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexpected_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1177\u001B[0m \u001B[43m        \u001B[49m\u001B[43metag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mxet_file_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxet_file_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1179\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1180\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(pointer_path):\n\u001B[0;32m   1181\u001B[0m         _create_symlink(blob_path, pointer_path, new_blob\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\huggingface_hub\\file_download.py:1735\u001B[0m, in \u001B[0;36m_download_to_tmp_and_move\u001B[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001B[0m\n\u001B[0;32m   1728\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m xet_file_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m constants\u001B[38;5;241m.\u001B[39mHF_HUB_DISABLE_XET:\n\u001B[0;32m   1729\u001B[0m             logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m   1730\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mXet Storage is enabled for this repo, but the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhf_xet\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m package is not installed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1731\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFalling back to regular HTTP download. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1732\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1733\u001B[0m             )\n\u001B[1;32m-> 1735\u001B[0m         \u001B[43mhttp_get\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1736\u001B[0m \u001B[43m            \u001B[49m\u001B[43murl_to_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1737\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1738\u001B[0m \u001B[43m            \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1739\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresume_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1740\u001B[0m \u001B[43m            \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1741\u001B[0m \u001B[43m            \u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexpected_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1742\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1744\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownload complete. Moving file to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdestination_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1745\u001B[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\huggingface_hub\\file_download.py:493\u001B[0m, in \u001B[0;36mhttp_get\u001B[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001B[0m\n\u001B[0;32m    491\u001B[0m new_resume_size \u001B[38;5;241m=\u001B[39m resume_size\n\u001B[0;32m    492\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 493\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m r\u001B[38;5;241m.\u001B[39miter_content(chunk_size\u001B[38;5;241m=\u001B[39mconstants\u001B[38;5;241m.\u001B[39mDOWNLOAD_CHUNK_SIZE):\n\u001B[0;32m    494\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m chunk:  \u001B[38;5;66;03m# filter out keep-alive new chunks\u001B[39;00m\n\u001B[0;32m    495\u001B[0m             progress\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mlen\u001B[39m(chunk))\n",
      "File \u001B[1;32mD:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\requests\\models.py:822\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[1;34m()\u001B[0m\n\u001B[0;32m    820\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    821\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 822\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m DecodeError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    824\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ContentDecodingError(e)\n",
      "\u001B[1;31mChunkedEncodingError\u001B[0m: ('Connection broken: IncompleteRead(4475246 bytes read, 1468636631 more expected)', IncompleteRead(4475246 bytes read, 1468636631 more expected))"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:29:17.267889Z",
     "start_time": "2025-11-27T14:29:17.188626Z"
    }
   },
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./causal_lm\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T14:29:23.561068Z",
     "start_time": "2025-11-27T14:29:23.526247Z"
    }
   },
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m      2\u001B[0m     args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m----> 3\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[43mmodel\u001B[49m,\n\u001B[0;32m      4\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer,\n\u001B[0;32m      5\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mtokenized_ds,\n\u001B[0;32m      6\u001B[0m     data_collator\u001B[38;5;241m=\u001B[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      7\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安\", max_length=128, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"下面是一则游戏新闻。小编报道，近日，游戏产业发展的非常\", max_length=128, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "两个文件的区别\n",
    "# masked_lm.ipynb - 掩码语言模型\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "\n",
    "# masked_lm: 使用MacBERT分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "\n",
    "\n",
    "# causal_lm.ipynb - 因果语言模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Langboat/bloom-389m-zh\")\n",
    "\n",
    "# causal_lm: 使用BLOOM分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/bloom-389m-zh\")\n",
    "\n",
    "\n",
    "方面\t        masked_lm.ipynb\t        causal_lm.ipynb\n",
    "模型架构\t        掩码语言模型 (MLM)\t因果语言模型 (CLM)\n",
    "训练目标\t        预测被掩码的词\t    预测下一个词\n",
    "注意力机制\t        双向注意力\t    单向注意力\n",
    "适用任务\t        文本理解、完形填空\t    文本生成、续写\n",
    "分词器\t        chinese-macbert-base\tbloom-389m-zh\n",
    "数据处理\t        直接分词\t            添加EOS Token\n",
    "DataCollator\tmlm=True\t        mlm=False\n",
    "推理管道\t        fill-mask\t        text-generation\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Transformers的命名实体识别"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:03.388134Z",
     "start_time": "2025-12-11T03:29:03.380183Z"
    }
   },
   "source": [
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:14.862797Z",
     "start_time": "2025-12-11T03:29:04.440409Z"
    }
   },
   "source": [
    "# 如果可以联网，直接使用load_dataset进行加载\n",
    "#ner_datasets = load_dataset(\"peoples_daily_ner\", cache_dir=\"./data\")\n",
    "# 如果无法联网，则使用下面的方式加载数据集\n",
    "#ner_datasets = DatasetDict.load_from_disk(\"ner_data\")\n",
    "\n",
    "from datasets import DatasetDict\n",
    "ner_datasets = load_dataset(\"peoples_daily_ner\", cache_dir=\"data\", trust_remote_code=True)\n",
    "ner_datasets"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 20865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 2319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 4637\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:17.873523Z",
     "start_time": "2025-12-11T03:29:17.853179Z"
    }
   },
   "source": [
    "'''对应关系：\n",
    "5 = B-LOC（地名开始）         6 = I-LOC（地名继续）\n",
    "识别的实体：\n",
    "\"厦门\"：[B-LOC, I-LOC]         \"金门\"：[B-LOC, I-LOC]\n",
    "'''\n",
    "ner_datasets[\"train\"][0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['海',\n",
       "  '钓',\n",
       "  '比',\n",
       "  '赛',\n",
       "  '地',\n",
       "  '点',\n",
       "  '在',\n",
       "  '厦',\n",
       "  '门',\n",
       "  '与',\n",
       "  '金',\n",
       "  '门',\n",
       "  '之',\n",
       "  '间',\n",
       "  '的',\n",
       "  '海',\n",
       "  '域',\n",
       "  '。'],\n",
       " 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:19.254658Z",
     "start_time": "2025-12-11T03:29:19.246174Z"
    }
   },
   "source": [
    "ner_datasets[\"train\"].features"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:20.361209Z",
     "start_time": "2025-12-11T03:29:20.349648Z"
    }
   },
   "source": [
    "label_list = ner_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\"\"\"\n",
    "B-：Begin，表示实体的开始,I-：Inside，表示实体的内部/继续,O：Outside，表示非实体\n",
    "O——非实体——\"的\"、\"在\"、\"和\"\n",
    "B-PE——人名开始——\"张\"（在\"张三\"中）\n",
    "\"\"\"\n",
    "label_list"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:22.083217Z",
     "start_time": "2025-12-11T03:29:21.529103Z"
    }
   },
   "source": "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:24.670422Z",
     "start_time": "2025-12-11T03:29:24.658498Z"
    }
   },
   "source": [
    "tokenizer(ner_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "# 对ner_datasets文件夹里的训练集里的第1个样本，提取该样本的\"tokens\"字段，这通常是一个已经分词好的列表，告诉tokenizer输入已经是分词好的单词列表\n",
    "'''\n",
    "101 = 开始符号（就像\"开始阅读\"）\n",
    "102 = 结束符号（就像\"阅读结束\"）\n",
    "\n",
    "第一个句子的所有token对应0，第二个句子的所有token对应1\n",
    "特殊标记[CLS]和[SEP]也会被赋予相应的句子ID。通常[CLS]和第一个句子后的[SEP]标记为0，第二个句子的token和最后的[SEP]标记为1\n",
    "    假设有两个句子[\"今天天气很好\", \"我们出去玩\"]，经过tokenizer后，可能变成：\n",
    "    tokens: [CLS] 今 天 天 气 很 好 [SEP] 我 们 出 去 玩 [SEP]\n",
    "    token_type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
    "\n",
    "当一批(batch)中的句子长度不一致时需要填充，填充后：所有句子变成相同长度。值：1=需要关注，0=忽略\n",
    "    '''"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n101 = 开始符号（就像\"开始阅读\"）\\n102 = 结束符号（就像\"阅读结束\"）\\n\\n第一个句子的所有token对应0，第二个句子的所有token对应1\\n特殊标记[CLS]和[SEP]也会被赋予相应的句子ID。通常[CLS]和第一个句子后的[SEP]标记为0，第二个句子的token和最后的[SEP]标记为1\\n    假设有两个句子[\"今天天气很好\", \"我们出去玩\"]，经过tokenizer后，可能变成：\\n    tokens: [CLS] 今 天 天 气 很 好 [SEP] 我 们 出 去 玩 [SEP]\\n    token_type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1\\n\\n当一批(batch)中的句子长度不一致时需要填充，填充后：所有句子变成相同长度。值：1=需要关注，0=忽略\\n    '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:26.102865Z",
     "start_time": "2025-12-11T03:29:26.090693Z"
    }
   },
   "source": [
    "res = tokenizer(\"interesting word\")\n",
    "#BERT按照子词分的，所以interesting word被分成了几个子词（具体怎么分不需要知道），所以内部是5个数字\n",
    "res\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 10673, 12865, 12921, 8181, 8681, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:27.182373Z",
     "start_time": "2025-12-11T03:29:27.173315Z"
    }
   },
   "source": "res.word_ids()#返回每个token对应原始文本中的单词索引，说明前四个数字是interesting,后一个是word",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 0, 0, 0, 1, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:28.375535Z",
     "start_time": "2025-12-11T03:29:28.358950Z"
    }
   },
   "source": [
    "# 借助word_ids 实现标签映射\n",
    "def process_function(examples):\n",
    "    tokenized_exmaples = tokenizer(examples[\"tokens\"], max_length=128, truncation=True, is_split_into_words=True)\n",
    "    '对example中的每一个名为\"token\"的字段进行分词，设置最大序列长度128，truncation=True如果超过128则截断，is_split_into_words=True告诉程序，输入（下载的数据集）已经分过词了，不要进一步分词'\n",
    "    'BERT要用的三个输入值：input_ids，attention_mask，token_type_ids'\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_exmaples.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)  #在计算损失时忽略\n",
    "            else:\n",
    "                label_ids.append(label[word_id])\n",
    "        labels.append(label_ids)\n",
    "    tokenized_exmaples[\"labels\"] = labels\n",
    "    return tokenized_exmaples"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:29.643736Z",
     "start_time": "2025-12-11T03:29:29.579723Z"
    }
   },
   "source": [
    "tokenized_datasets = ner_datasets.map(process_function, batched=True)\n",
    "'''\n",
    ".map()\n",
    "功能：对数据集中的每个样本应用指定的处理函数\n",
    "返回：一个新的数据集，包含处理后的结果\n",
    "'''\n",
    "tokenized_datasets"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4637\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:31.115777Z",
     "start_time": "2025-12-11T03:29:31.098331Z"
    }
   },
   "source": [
    "print(tokenized_datasets[\"train\"][0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间', '的', '海', '域', '。'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305, 680, 7032, 7305, 722, 7313, 4638, 3862, 1818, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0, -100]}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:41.363958Z",
     "start_time": "2025-12-11T03:29:39.656627Z"
    }
   },
   "source": [
    "# 对于所有的非二分类任务，切记要指定num_labels，否则就会device错误\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"hfl/chinese-macbert-base\", num_labels=len(label_list))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:43.125754Z",
     "start_time": "2025-12-11T03:29:43.108079Z"
    }
   },
   "source": [
    "model.config.num_labels"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 创建评估函数"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:44.217946Z",
     "start_time": "2025-12-11T03:29:44.182398Z"
    }
   },
   "source": [
    "# 这里方便加载，替换成了本地的加载方式，无需额外下载\n",
    "seqeval = evaluate.load(\"seqeval_metric.py\")\n",
    "seqeval"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"seqeval\", module_type: \"metric\", features: {'predictions': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence')}, usage: \"\"\"\n",
       "Produces labelling scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n",
       "    references: List of List of reference labels (Ground truth (correct) target values)\n",
       "    suffix: True if the IOB prefix is after type, False otherwise. default: False\n",
       "    scheme: Specify target tagging scheme. Should be one of [\"IOB1\", \"IOB2\", \"IOE1\", \"IOE2\", \"IOBES\", \"BILOU\"].\n",
       "        default: None\n",
       "    mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n",
       "        If you want to only count exact matches, pass mode=\"strict\". default: None.\n",
       "    sample_weight: Array-like of shape (n_samples,), weights for individual samples. default: None\n",
       "    zero_division: Which value to substitute as a metric value when encountering zero division. Should be on of 0, 1,\n",
       "        \"warn\". \"warn\" acts as 0, but the warning is raised.\n",
       "\n",
       "Returns:\n",
       "    'scores': dict. Summary of the scores for overall and per type\n",
       "        Overall:\n",
       "            'accuracy': accuracy,\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure,\n",
       "        Per type:\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> seqeval = evaluate.load(\"seqeval\")\n",
       "    >>> results = seqeval.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['MISC', 'PER', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n",
       "    >>> print(results[\"overall_f1\"])\n",
       "    0.5\n",
       "    >>> print(results[\"PER\"][\"f1\"])\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:45.403826Z",
     "start_time": "2025-12-11T03:29:45.397662Z"
    }
   },
   "source": [
    "#这个函数接收模型预测结果，计算NER任务的F1分数。\n",
    "import numpy as np\n",
    "\n",
    "def eval_metric(pred):\n",
    "    predictions, labels = pred\n",
    "    'pred 是模型输出的预测结果和真实标签,predictions: 模型的原始输出（通常是logits）,labels: 真实的标签序列'\n",
    "    predictions = np.argmax(predictions, axis=-1)   #返回该行/列（axis=0，看每一列，axis=-1，从倒一行开始看）最大索引，\n",
    "    # 在二维数组中，axis=-1和axis=1是一样的，只不过从倒数第一行开始看\n",
    "\n",
    "    # 将id转换为原始的字符串类型的标签\n",
    "    true_predictions = [\n",
    "        [label_list[p] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels) \n",
    "    ]\n",
    "\n",
    "    '''\n",
    "    在PyTorch中，-100 是交叉熵损失函数的默认\"忽略索引\".当标签为-100时，该位置的损失不会被计算，梯度也不会回传（被忽略）。\n",
    "    具体哪些位置会被标记为-100？\n",
    "        [CLS] token：序列开始标记\n",
    "        [SEP] token：序列结束标记\n",
    "        Padding tokens：填充位置（如果序列长度不足max_length）\n",
    "        子词的后续部分：一个单词被拆成多个子词时，只有第一个子词保留标签，后续子词标记为-100\n",
    "    '''\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels) \n",
    "    ]\n",
    "\n",
    "    result = seqeval.compute(predictions=true_predictions, references=true_labels, mode=\"strict\", scheme=\"IOB2\")\n",
    "\n",
    "    return {\n",
    "        \"f1\": result[\"overall_f1\"]\n",
    "    }\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:47.691485Z",
     "start_time": "2025-12-11T03:29:47.609076Z"
    }
   },
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"models_for_ner\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=128,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,   #每过50个batch打印一次\n",
    "    num_train_epochs=1\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:29:50.983374Z",
     "start_time": "2025-12-11T03:29:50.631815Z"
    }
   },
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=eval_metric,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T05:06:20.133077Z",
     "start_time": "2025-11-17T04:55:13.944683Z"
    }
   },
   "source": [
    "trainer.train()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\llm-project\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='327' max='327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [327/327 11:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.020442</td>\n",
       "      <td>0.940093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "d1e2d1bb4d1246fe82b361a27f7dd754"
     }
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=327, training_loss=0.06583504757020817, metrics={'train_runtime': 665.6072, 'train_samples_per_second': 31.347, 'train_steps_per_second': 0.491, 'total_flos': 1317626511207666.0, 'train_loss': 0.06583504757020817, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T05:07:26.270333Z",
     "start_time": "2025-11-17T05:06:20.232599Z"
    }
   },
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37/37 01:02]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "37496b1373dffa54a2e4f837739f4e3e"
     }
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.024249466136097908,\n",
       " 'eval_f1': 0.9311757201369771,\n",
       " 'eval_runtime': 66.0139,\n",
       " 'eval_samples_per_second': 70.243,\n",
       " 'eval_steps_per_second': 0.56,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T05:07:26.364798Z",
     "start_time": "2025-11-17T05:07:26.354194Z"
    }
   },
   "source": [
    "from transformers import pipeline"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T05:07:26.380247Z",
     "start_time": "2025-11-17T05:07:26.372034Z"
    }
   },
   "source": [
    "# 使用pipeline进行推理，要指定id2label\n",
    "model.config.id2label = {idx: label for idx, label in enumerate(label_list)}\n",
    "model.config"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"hfl/chinese-macbert-base\",\n",
       "  \"architectures\": [\n",
       "    \"BertForTokenClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"O\",\n",
       "    \"1\": \"B-PER\",\n",
       "    \"2\": \"I-PER\",\n",
       "    \"3\": \"B-ORG\",\n",
       "    \"4\": \"I-ORG\",\n",
       "    \"5\": \"B-LOC\",\n",
       "    \"6\": \"I-LOC\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.42.4\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T05:07:26.411127Z",
     "start_time": "2025-11-17T05:07:26.393533Z"
    }
   },
   "source": [
    "# 如果模型是基于GPU训练的，那么推理时要指定device\n",
    "# 对于NER任务，可以指定aggregation_strategy为simple，得到具体的实体的结果，而不是token的结果\n",
    "ner_pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, device=0, aggregation_strategy=\"simple\")"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T05:07:26.457909Z",
     "start_time": "2025-11-17T05:07:26.418549Z"
    }
   },
   "source": [
    "res = ner_pipe(\"小明在北京上班\")\n",
    "res"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.94918025,\n",
       "  'word': '小 明',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9956497,\n",
       "  'word': '北 京',\n",
       "  'start': 3,\n",
       "  'end': 5}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T05:07:26.488938Z",
     "start_time": "2025-11-17T05:07:26.471456Z"
    }
   },
   "source": [
    "# 根据start和end取实际的结果\n",
    "ner_result = {}\n",
    "x = \"小明在北京上班\"\n",
    "for r in res:\n",
    "    if r[\"entity_group\"] not in ner_result:\n",
    "        ner_result[r[\"entity_group\"]] = []\n",
    "    ner_result[r[\"entity_group\"]].append(x[r[\"start\"]: r[\"end\"]])\n",
    "\n",
    "ner_result"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PER': ['小明'], 'LOC': ['北京']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T05:07:26.504587Z",
     "start_time": "2025-11-17T05:07:26.496467Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "B-：Begin，表示实体的开始    I-：Inside，表示实体的内部/继续\n",
    "O：Outside，表示非实体\n",
    "\n",
    "O\t     非实体\t\"的\"、\"在\"、\"和\"\n",
    "B-PER\t人名开始\t\"张\"（在\"张三\"中）\n",
    "I-PER\t人名继续\t\"三\"（在\"张三\"中）\n",
    "B-ORG\t组织机构开始\t\"阿里\"（在\"阿里巴巴\"中）\n",
    "I-ORG\t组织机构继续\t\"巴巴\"（在\"阿里巴巴\"中）\n",
    "B-LOC\t地名开始\t\"北\"（在\"北京\"中）\n",
    "I-LOC\t地名继续\t\"京\"（在\"北京\"中）\n",
    "\n",
    "例如：\n",
    "张      三   在   北     京   的   阿        里   巴   巴   工\n",
    "B-PER I-PER O   B-LOC I-LOC O   B-ORG I-ORG I-ORG I-ORG O\n",
    " 作\n",
    " O\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
